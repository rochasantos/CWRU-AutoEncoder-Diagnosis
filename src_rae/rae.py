import torch
import torch.nn as nn

class RAE(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, latent_dim=16, num_layers=2):
        super(RAE, self).__init__()

        # ğŸ”¹ Encoder (LSTM)
        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)  # MÃ©dia do espaÃ§o latente
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # Log da variÃ¢ncia

        # ğŸ”¹ Decoder (LSTM)
        self.fc_dec = nn.Linear(latent_dim, hidden_dim)  # Projeta para o espaÃ§o oculto do LSTM
        self.decoder = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)  # Desvio padrÃ£o
        eps = torch.randn_like(std)  # RuÃ­do gaussiano
        return mu + eps * std  # Amostragem do espaÃ§o latente

    def forward(self, x):
        batch_size, seq_len, input_dim = x.shape

        # ğŸ”¹ Passa pelo encoder LSTM
        _, (h_n, _) = self.encoder(x)
        encoded = h_n[-1]  # Usa apenas a Ãºltima saÃ­da do LSTM

        # ğŸ”¹ Calcula mÃ©dia e variÃ¢ncia para o espaÃ§o latente
        mu = self.fc_mu(encoded)
        logvar = self.fc_logvar(encoded)
        z = self.reparameterize(mu, logvar)

        # ğŸ”¹ DecodificaÃ§Ã£o
        hidden = self.fc_dec(z).unsqueeze(0).repeat(self.decoder.num_layers, 1, 1)
        decoded, _ = self.decoder(hidden)

        return decoded, mu, logvar
